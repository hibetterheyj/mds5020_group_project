# Task2-Subtask1ï¼ˆé‡‘èæ–°é—»æƒ…æ„Ÿåˆ†æï¼‰ä¼˜åŒ–æ–¹æ¡ˆä¸è½»é‡éƒ¨ç½²æ¨è

## doubao

ç»“åˆä½ çš„å®éªŒç°çŠ¶ï¼ˆä¼ ç»ŸMLæ¨¡å‹æ•ˆæœâ‰ˆ81%ï¼Œæ‰‹å·¥ç‰¹å¾æå‡æœ‰é™ï¼Œéœ€æ»¡è¶³è½»é‡éƒ¨ç½²çº¦æŸï¼‰ï¼Œä»¥ä¸‹ä»**æ•ˆæœä¼˜åŒ–**ã€**è½»é‡æ¨¡å‹æ¨è**ã€**éƒ¨ç½²è½åœ°**ä¸‰ä¸ªç»´åº¦æä¾›ç³»ç»Ÿæ€§æ–¹æ¡ˆï¼Œä¼˜å…ˆä¿è¯ã€Œèµ„æºçº¦æŸã€ï¼ˆå®¹å™¨å†…å­˜â‰¤900MBã€é•œåƒâ‰¤4GBï¼‰å’Œã€Œé¢„æµ‹é€Ÿåº¦ã€ï¼ˆå 5%è¯„åˆ†ï¼‰ï¼ŒåŒæ—¶æœ€å¤§åŒ–ã€ŒåŠ æƒF1å¾—åˆ†ã€ï¼ˆå 20%è¯„åˆ†ï¼‰ã€‚


### ä¸€ã€ç°çŠ¶è¯Šæ–­ä¸æ ¸å¿ƒé—®é¢˜
é¦–å…ˆæ˜ç¡®å½“å‰å®éªŒçš„å…³é”®ç»“è®ºï¼Œä¸ºåç»­ä¼˜åŒ–æä¾›ä¾æ®ï¼š
1. **æ¨¡å‹è¡¨ç°**ï¼šçº¿æ€§æ¨¡å‹ï¼ˆLogistic Regressionã€LinearSVCï¼‰æ•ˆæœæœ€ä¼˜ï¼ˆâ‰ˆ81%ï¼‰ï¼Œé›†æˆæ¨¡å‹ï¼ˆXGBoost/LightGBMï¼‰åè€Œæ›´ä½ï¼ˆâ‰ˆ77%ï¼‰ï¼ŒåŸå› æ˜¯æ–‡æœ¬æ•°æ®ï¼ˆ4300æ¡ï¼‰é‡è¾ƒå°ï¼Œé›†æˆæ¨¡å‹æ˜“è¿‡æ‹Ÿåˆï¼Œçº¿æ€§æ¨¡å‹æ›´ç¨³å¥ã€‚
2. **æ‰‹å·¥ç‰¹å¾ä»·å€¼**ï¼šæå‡å¹…åº¦ä»…0.15%-0.36%ï¼Œé—®é¢˜åœ¨äºç‰¹å¾è®¾è®¡åé€šç”¨ï¼ˆå¦‚â€œprofitâ€â€œlossâ€ï¼‰ï¼Œæœªç»“åˆé‡‘èæ–‡æœ¬çš„**é¢†åŸŸç‰¹å¼‚æ€§**ï¼Œä¸”ç‰¹å¾ç»´åº¦å•ä¸€ã€‚
3. **è¿‡æ‹Ÿåˆé£é™©**ï¼š5æŠ˜äº¤å‰éªŒè¯æ ‡å‡†å·®ï¼ˆ0.006-0.014ï¼‰è¾ƒå°ï¼Œæ¨¡å‹ç¨³å®šæ€§è‰¯å¥½ï¼Œæ— æ˜æ˜¾è¿‡æ‹Ÿåˆï¼›è‹¥éœ€è¿›ä¸€æ­¥è§„é¿ï¼Œå¯é€šè¿‡æ­£åˆ™åŒ–ã€ç‰¹å¾é€‰æ‹©ä¼˜åŒ–ã€‚
4. **éƒ¨ç½²ä¼˜åŠ¿**ï¼šç°æœ‰çº¿æ€§æ¨¡å‹ï¼ˆLogReg/LinearSVCï¼‰æœ¬èº«è½»é‡ï¼ˆæ¨¡å‹æ–‡ä»¶KBçº§ï¼Œæ¨ç†å†…å­˜å‡ åMBï¼‰ï¼Œå®Œå…¨æ»¡è¶³éƒ¨ç½²çº¦æŸï¼Œæ— éœ€æ›´æ¢â€œé‡æ¨¡å‹â€ã€‚


### äºŒã€ä¼˜å…ˆæ¨èï¼šä¼˜åŒ–ç°æœ‰çº¿æ€§æ¨¡å‹ï¼ˆæˆæœ¬æœ€ä½ï¼Œæ•ˆæœç«‹ç«¿è§å½±ï¼‰
ä½ çš„æ ¸å¿ƒç›®æ ‡æ˜¯ã€Œæå‡F1+ä¿æŒè½»é‡ã€ï¼Œ**ä¼˜åŒ–ç°æœ‰çº¿æ€§æ¨¡å‹+ç‰¹å¾å·¥ç¨‹**æ˜¯æŠ•å…¥äº§å‡ºæ¯”æœ€é«˜çš„æ–¹æ¡ˆï¼Œæ— éœ€å¼•å…¥æ–°ä¾èµ–ï¼Œéƒ¨ç½²æˆæœ¬ä¸ºé›¶ã€‚


#### 2.1 ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–ï¼ˆå…³é”®çªç ´ç‚¹ï¼‰
æ‰‹å·¥ç‰¹å¾æå‡æœ‰é™çš„æ ¸å¿ƒæ˜¯ã€Œé€šç”¨æƒ…æ„Ÿè¯+å•ä¸€ç»´åº¦ã€ï¼Œéœ€ç»“åˆé‡‘èæ–‡æœ¬ç‰¹æ€§è®¾è®¡**ç²¾å‡†ç‰¹å¾**ï¼Œä»¥ä¸‹æ˜¯å¯è½åœ°çš„æ”¹è¿›æ–¹æ¡ˆï¼š

##### ï¼ˆ1ï¼‰æ‰©å……é‡‘èé¢†åŸŸæƒ…æ„Ÿè¯åº“
ç°æœ‰æ­£è´Ÿè¯åº“åé€šç”¨ï¼Œéœ€è¡¥å……é‡‘èä¸“å±æœ¯è¯­ï¼ˆåŸºäºä½ çš„EDAé«˜é¢‘è¯å’Œé‡‘èå¸¸è¯†ï¼‰ï¼š
```python
# ä¼˜åŒ–åçš„æƒ…æ„Ÿè¯åº“ï¼ˆæ›¿æ¢handcrafted_features.pyä¸­çš„positive/negative_wordsï¼‰
positive_words = [
    # åŸæœ‰åŸºç¡€è¯
    'profit', 'rise', 'increase', 'growth', 'higher', 'gain',
    'win', 'success', 'improve', 'boost', 'surge', 'soar',
    'strong', 'positive', 'beat', 'exceed',
    # æ–°å¢é‡‘èä¸“å±æ­£é¢è¯
    'eps', 'roe', 'dividend', 'revenue', 'hike', 'outperform',
    'upgrade', 'beat_forecast', 'above_expected', 'profit_margin_up',
    'sales_growth', 'cash_flow_positive', 'dividend_increase'
]

negative_words = [
    # åŸæœ‰åŸºç¡€è¯
    'loss', 'fall', 'decrease', 'drop', 'cut', 'lower',
    'decline', 'weak', 'negative', 'miss', 'fail', 'warn',
    'lose', 'fell', 'downgrade',
    # æ–°å¢é‡‘èä¸“å±è´Ÿé¢è¯
    'loss_widen', 'revenue_miss', 'dividend_cut', 'eps_drop',
    'downgrade', 'underperform', 'below_expected', 'profit_margin_down',
    'sales_slump', 'cash_flow_negative', 'default_risk'
]
```

##### ï¼ˆ2ï¼‰æ–°å¢3ç±»é«˜ä»·å€¼ç‰¹å¾
åŸºäºé‡‘èæ–‡æœ¬çš„ã€Œæ•°å­—æƒ…æ„Ÿã€ã€Œè¯­å¢ƒä½ç½®ã€ã€Œé¢†åŸŸå¯†åº¦ã€è®¾è®¡ç‰¹å¾ï¼Œæå‡åŒºåˆ†åº¦ï¼š
```python
def create_sentiment_features(df):
    """ä¼˜åŒ–åçš„æ‰‹å·¥ç‰¹å¾å‡½æ•°ï¼Œæ–°å¢3ç±»ç‰¹å¾"""
    positive_words = [...]  # ä¸Šè¿°æ‰©å……è¯åº“
    negative_words = [...]
    financial_terms = ['eur', 'usd', 'gbp', 'eps', 'roe', 'dividend', 'revenue', 'sales', 'profit', 'loss']  # é‡‘èä¸“å±æœ¯è¯­

    features = []
    for text in df['news_title']:
        text_lower = str(text).lower()
        words = text_lower.split()

        # 1. åŸæœ‰åŸºç¡€ç‰¹å¾ï¼ˆä¿ç•™å¹¶ä¼˜åŒ–ï¼‰
        pos_count = sum(1 for word in positive_words if word in text_lower)
        neg_count = sum(1 for word in negative_words if word in text_lower)
        total_sentiment_words = pos_count + neg_count
        pos_ratio = pos_count / (total_sentiment_words + 1e-10)
        neg_ratio = neg_count / (total_sentiment_words + 1e-10)
        net_sentiment = pos_count - neg_count
        has_strong_positive = int(any(w in text_lower for w in ['soar', 'surge', 'beat_forecast']))
        has_strong_negative = int(any(w in text_lower for w in ['plunge', 'crash', 'loss_widen']))

        # 2. æ–°å¢ï¼šé‡‘èæœ¯è¯­å¯†åº¦ï¼ˆç»†åˆ†é¢†åŸŸè¯ï¼Œæ¯”åŸfinancial_densityæ›´ç²¾å‡†ï¼‰
        finance_count = sum(1 for term in financial_terms if term in text_lower)
        finance_density = finance_count / (len(words) + 1e-10)

        # 3. æ–°å¢ï¼šæ•°å­—æƒ…æ„Ÿç‰¹å¾ï¼ˆé‡‘èæ–‡æœ¬ä¸­æ•°å­—+è¶‹åŠ¿è¯æ˜¯å¼ºä¿¡å·ï¼‰
        has_pos_num = int(any(re.search(r'(up|rise|increase|higher) \d+%', text_lower) or
                              re.search(r'\d+% (up|rise)', text_lower)))
        has_neg_num = int(any(re.search(r'(down|fall|decrease|lower) \d+%', text_lower) or
                              re.search(r'\d+% (down|fall)', text_lower)))

        # 4. æ–°å¢ï¼šæƒ…æ„Ÿè¯ä½ç½®ç‰¹å¾ï¼ˆå¥é¦–/å¥å°¾çš„æƒ…æ„Ÿè¯æƒé‡æ›´é«˜ï¼‰
        pos_word_at_start = int(words[0] in positive_words) if len(words) > 0 else 0
        neg_word_at_end = int(words[-1] in negative_words) if len(words) > 0 else 0

        features.append([
            pos_count, neg_count, pos_ratio, neg_ratio, net_sentiment,
            has_strong_positive, has_strong_negative,
            finance_count, finance_density,  # ä¼˜åŒ–+æ–°å¢é¢†åŸŸç‰¹å¾
            has_pos_num, has_neg_num,        # æ–°å¢æ•°å­—æƒ…æ„Ÿç‰¹å¾
            pos_word_at_start, neg_word_at_end  # æ–°å¢ä½ç½®ç‰¹å¾
        ])

    # æ–°å¢ç‰¹å¾åç§°
    feature_names = [
        'pos_word_count', 'neg_word_count', 'pos_ratio', 'neg_ratio', 'net_sentiment',
        'has_strong_positive', 'has_strong_negative',
        'finance_term_count', 'finance_term_density',
        'has_positive_number', 'has_negative_number',
        'pos_word_at_start', 'neg_word_at_end'
    ]
    return pd.DataFrame(features, columns=feature_names)
```

##### ï¼ˆ3ï¼‰æ–‡æœ¬é¢„å¤„ç†åŠ å¼ºï¼ˆé’ˆå¯¹é‡‘èæ–‡æœ¬ç‰¹æ€§ï¼‰
ä¿®æ”¹`text_preprocessor.py`ï¼Œä¼˜åŒ–é‡‘èé¢†åŸŸç‰¹æ®Šè¡¨è¾¾çš„å¤„ç†ï¼ˆå¦‚â€œeur100mâ€â€œ5%growthâ€ï¼‰ï¼š
```python
def preprocess(self, text, max_words=100):
    if not isinstance(text, str) or not text.strip():
        return ""
    text = text.lower()

    # æ–°å¢ï¼šå¤„ç†é‡‘èæ•°å­—æ ¼å¼ï¼ˆå¦‚â€œeur100mâ€â†’â€œeur 100mâ€ï¼Œâ€œ5%growthâ€â†’â€œ5% growthâ€ï¼‰
    text = re.sub(r'([a-z]+)(\d+[a-z%])', r'\1 \2', text)  # å­—æ¯+æ•°å­—ï¼ˆå¦‚eur100mâ†’eur 100mï¼‰
    text = re.sub(r'(\d+%)([a-z]+)', r'\1 \2', text)       # æ•°å­—%+å­—æ¯ï¼ˆå¦‚5%growthâ†’5% growthï¼‰

    # åŸæœ‰å¤„ç†ä¿ç•™
    text = re.sub(r'\b(\d+)\s*percent\b', r'\1percent', text)
    text = re.sub(r'\b(\d+)\s*pct\b', r'\1percent', text)
    text = re.sub(r'\$(\d+(?:\.\d+)?)\s*million\b', r'dollar\1m', text, flags=re.IGNORECASE)
    text = re.sub(r'\$(\d+(?:\.\d+)?)\s*billion\b', r'dollar\1b', text, flags=re.IGNORECASE)
    text = re.sub(r'[^\w\s\.\,\!\?\-\+\%\$\&]', ' ', text)

    words = text.split()
    if len(words) > max_words:
        words = words[:max_words]

    # æ–°å¢ï¼šä¿ç•™é‡‘èæœ¯è¯­ï¼ˆé¿å…è¢«åœç”¨è¯è¿‡æ»¤ï¼Œå¦‚â€œepsâ€â€œroeâ€ï¼‰
    financial_terms = {'eur', 'usd', 'gbp', 'eps', 'roe', 'dividend', 'revenue', 'sales'}
    processed_words = []
    for word in words:
        if len(word) < 2 and word not in {'no', 'up', 'in', 'on', 'at', 'to', 'by'}:
            continue
        if word in self.extended_stopwords and word not in financial_terms:  # é‡‘èæœ¯è¯­ä¸è¢«åœç”¨
            continue
        # åŸæœ‰è¯å½¢è¿˜åŸ/è¯å¹²æå–ä¿ç•™
        if self.lemmatizer:
            word = self.lemmatizer.lemmatize(word, pos='n')
            word = self.lemmatizer.lemmatize(word, pos='v')
            word = self.lemmatizer.lemmatize(word, pos='a')
        if self.stemmer:
            word = self.stemmer.stem(word)
        processed_words.append(word)
    return ' '.join(processed_words)
```

##### ï¼ˆ4ï¼‰ç‰¹å¾é€‰æ‹©ï¼ˆå‡å°‘å†—ä½™ï¼Œé¿å…è¿‡æ‹Ÿåˆï¼‰
åœ¨æ¨¡å‹ pipeline ä¸­åŠ å…¥**ç‰¹å¾é€‰æ‹©æ­¥éª¤**ï¼ˆå¦‚`SelectKBest`ï¼‰ï¼Œç­›é€‰é«˜ä¿¡æ¯é‡ç‰¹å¾ï¼Œæå‡æ¨¡å‹æ•ˆç‡ï¼š
```python
# ä»¥Logistic Regressionä¸ºä¾‹ï¼Œä¿®æ”¹create_logregression_pipelineå‡½æ•°
from sklearn.feature_selection import SelectKBest, chi2

def create_logregression_pipeline(use_handcrafted: bool = True) -> Pipeline:
    if use_handcrafted:
        feature_union = FeatureUnion([
            ('text', Pipeline([
                ('selector', FunctionTransformer(get_text_column, validate=False)),
                ('preprocessor', TextPreprocessorTransformer()),
                ('tfidf', TfidfVectorizer(
                    max_features=5000,  # å…ˆä¿ç•™æ›´å¤šç‰¹å¾ï¼Œå†ç­›é€‰
                    ngram_range=(1, 3),
                    min_df=2,  # è¿‡æ»¤ä½é¢‘è¯ï¼ˆå‡ºç°<2æ¬¡çš„è¯ï¼‰
                    use_idf=True
                )),
                ('select_k', SelectKBest(chi2, k=3000))  # ç­›é€‰Top3000 TF-IDFç‰¹å¾
            ])),
            ('handcrafted', Pipeline([
                ('selector', FunctionTransformer(get_handcrafted_features, validate=False)),
                ('scaler', StandardScaler()),
                ('select_k_hand', SelectKBest(chi2, k=10))  # ç­›é€‰Top10æ‰‹å·¥ç‰¹å¾
            ]))
        ])
        pipeline = Pipeline([
            ('features', feature_union),
            ('classifier', LogisticRegression(
                C=2.0,  # é€‚å½“å‡å°Cï¼Œå¢å¼ºæ­£åˆ™åŒ–
                penalty='l1',
                solver='liblinear',
                class_weight='balanced',
                random_state=42
            ))
        ])
    else:
        # æ–‡æœ¬-only pipeline ç±»ä¼¼ï¼ŒåŠ å…¥SelectKBest
        ...
    return pipeline
```


#### 2.2 çº¿æ€§æ¨¡å‹å‚æ•°ç²¾è°ƒ
åŸºäºä¼˜åŒ–åçš„ç‰¹å¾ï¼Œè¿›ä¸€æ­¥æ‰©å¤§å‚æ•°æœç´¢èŒƒå›´ï¼ˆä»¥Logistic Regressionä¸ºä¾‹ï¼‰ï¼š
```python
# ä¿®æ”¹tune_logregression_hyperparametersä¸­çš„param_grid
param_grid = {
    'classifier__C': np.logspace(-3, 3, 30),  # æ›´ç»†çš„CèŒƒå›´ï¼ˆ0.001~1000ï¼‰
    'classifier__penalty': ['l1', 'l2'],
    'classifier__max_iter': [500, 1000],
    'classifier__class_weight': [None, 'balanced'],
    'features__text__tfidf__max_features': [4000, 5000, 6000],
    'features__text__select_k__k': [2500, 3000, 3500],  # ç‰¹å¾é€‰æ‹©çš„kå€¼ä¹Ÿä½œä¸ºå‚æ•°
    'features__handcrafted__select_k_hand__k': [8, 10, 12]
}
```

**é¢„æœŸæ•ˆæœ**ï¼šä¼˜åŒ–åLogistic Regression/LinearSVCçš„åŠ æƒF1å¯æå‡è‡³**82%-83%** ï¼Œä¸”æ¨¡å‹æ›´ç®€æ´ï¼Œæ¨ç†é€Ÿåº¦ä¸å˜ï¼ˆä»ä¸ºæ¯«ç§’çº§ï¼‰ã€‚


### ä¸‰ã€æ¬¡é€‰æ–¹æ¡ˆï¼šè½»é‡çº§é›†æˆæ¨¡å‹ï¼ˆVoting Classifierï¼‰
è‹¥ä¼˜åŒ–çº¿æ€§æ¨¡å‹åæ•ˆæœä»æœªè¾¾é¢„æœŸï¼Œå¯é€šè¿‡**é›†æˆå¤šä¸ªè½»é‡çº¿æ€§æ¨¡å‹**è¿›ä¸€æ­¥æå‡ç¨³å®šæ€§å’Œæ•ˆæœï¼Œä¸”ä¸å¢åŠ å¤ªå¤šèµ„æºå ç”¨ã€‚


#### 3.1 é›†æˆæ€è·¯
é€‰æ‹©3ä¸ªè¡¨ç°æœ€ä¼˜çš„è½»é‡åŸºç¡€æ¨¡å‹ï¼ˆå‡ä¸ºçº¿æ€§æ¨¡å‹ï¼Œé¿å…è¿‡æ‹Ÿåˆï¼‰ï¼Œé‡‡ç”¨**è½¯æŠ•ç¥¨**ï¼ˆåŸºäºæ¦‚ç‡å¹³å‡ï¼‰æå‡æ•ˆæœï¼š
- åŸºç¡€æ¨¡å‹1ï¼šä¼˜åŒ–åçš„Logistic Regressionï¼ˆF1â‰ˆ82%ï¼‰
- åŸºç¡€æ¨¡å‹2ï¼šLinearSVCï¼ˆéœ€å¼€å¯`probability=True`ï¼Œæ”¯æŒæ¦‚ç‡è¾“å‡ºï¼‰
- åŸºç¡€æ¨¡å‹3ï¼šSGDClassifierï¼ˆloss='log_loss'ï¼Œå³é€»è¾‘å›å½’çš„SGDç‰ˆæœ¬ï¼Œæ›´è½»é‡ï¼‰


#### 3.2 ä»£ç å®ç°
```python
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import LinearSVC

# 1. å®šä¹‰3ä¸ªåŸºç¡€æ¨¡å‹ï¼ˆå‡ä¸ºä¼˜åŒ–åçš„ç‰ˆæœ¬ï¼‰
model1 = LogisticRegression(
    C=2.0, penalty='l1', solver='liblinear', class_weight='balanced', random_state=42
)
model2 = LinearSVC(
    C=0.5, penalty='l2', dual=False, class_weight='balanced', probability=True, random_state=42
)
model3 = SGDClassifier(
    loss='log_loss', penalty='l1', alpha=0.001, class_weight='balanced', max_iter=1000, random_state=42
)

# 2. æ„å»ºVoting Classifierï¼ˆè½¯æŠ•ç¥¨ï¼‰
voting_clf = VotingClassifier(
    estimators=[
        ('logreg', model1),
        ('linearsvc', model2),
        ('sgd', model3)
    ],
    voting='soft',  # è½¯æŠ•ç¥¨ï¼ˆåŸºäºæ¦‚ç‡å¹³å‡ï¼‰
    weights=[1, 1, 1]  # æƒé‡ç›¸ç­‰ï¼Œå¯æ ¹æ®æ¨¡å‹æ•ˆæœè°ƒæ•´
)

# 3. é›†æˆåˆ°pipelineï¼ˆä¸åŸæœ‰ç‰¹å¾å¤„ç†ç»“åˆï¼‰
def create_voting_pipeline() -> Pipeline:
    feature_union = FeatureUnion([  # å¤ç”¨ä¼˜åŒ–åçš„ç‰¹å¾å¤„ç†
        ('text', Pipeline([...])),  # åŒ2.1ä¸­çš„text pipeline
        ('handcrafted', Pipeline([...]))  # åŒ2.1ä¸­çš„æ‰‹å·¥ç‰¹å¾pipeline
    ])
    pipeline = Pipeline([
        ('features', feature_union),
        ('classifier', voting_clf)
    ])
    return pipeline
```


#### 3.3 èµ„æºä¸æ•ˆæœåˆ†æ
- **èµ„æºå ç”¨**ï¼š3ä¸ªçº¿æ€§æ¨¡å‹çš„æ€»å†…å­˜å ç”¨ä»â‰¤100MBï¼ˆæ¨¡å‹æ–‡ä»¶KBçº§ï¼‰ï¼Œæ¨ç†æ—¶ä»…éœ€ä¾æ¬¡è®¡ç®—3ä¸ªæ¨¡å‹çš„æ¦‚ç‡å¹¶å¹³å‡ï¼Œé€Ÿåº¦æ¯”å•ä¸ªæ¨¡å‹æ…¢2-3å€ï¼ˆä½†ä»ä¸ºæ¯«ç§’çº§ï¼Œ1ä¸‡æ¡æ•°æ®é¢„æµ‹æ—¶é—´â‰¤30ç§’ï¼‰ã€‚
- **æ•ˆæœé¢„æœŸ**ï¼šåŠ æƒF1å¯æå‡è‡³**83%-84%** ï¼Œ5æŠ˜äº¤å‰æ ‡å‡†å·®â‰¤0.01ï¼Œç¨³å®šæ€§ä¼˜äºå•ä¸ªæ¨¡å‹ã€‚


### å››ã€è¿›é˜¶æ–¹æ¡ˆï¼šè½»é‡çº§é¢„è®­ç»ƒæ¨¡å‹ï¼ˆæ•ˆæœè·ƒå‡ï¼‰
è‹¥éœ€è¿›ä¸€æ­¥çªç ´85%+çš„F1ï¼Œå¯å¼•å…¥**DistilBERT Tiny**ï¼ˆè½»é‡çº§é¢„è®­ç»ƒæ¨¡å‹ï¼‰ï¼Œä¸“ä¸ºæƒ…æ„Ÿåˆ†æä¼˜åŒ–ï¼Œä¸”æ»¡è¶³èµ„æºçº¦æŸã€‚


#### 4.1 æ¨¡å‹é€‰æ‹©ç†ç”±
- **ä½“ç§¯å°**ï¼šDistilBERT Tinyï¼ˆå¦‚`distilbert-base-uncased-finetuned-sst-2-english`ï¼‰ä½“ç§¯ä»…â‰ˆ100MBï¼Œè¿œå°äºBERTï¼ˆ400MB+ï¼‰ã€‚
- **é€Ÿåº¦å¿«**ï¼šæ¨ç†é€Ÿåº¦æ¯”BERTå¿«60%ï¼ŒCPUå•æ¬¡æ¨ç†â‰¤20ms/æ¡ã€‚
- **æ•ˆæœå¥½**ï¼šé¢„è®­ç»ƒæ¨¡å‹èƒ½ç†è§£ä¸Šä¸‹æ–‡ï¼ˆå¦‚â€œprofit fell less than expectedâ€è¿™ç±»æ­§ä¹‰å¥ï¼‰ï¼Œé‡‘èæƒ…æ„Ÿåˆ†æF1å¯è¾¾85%-88%ã€‚


#### 4.2 éƒ¨ç½²èµ„æºéªŒè¯
| èµ„æºé¡¹                | çº¦æŸè¦æ±‚       | å®é™…å ç”¨       | æ»¡è¶³æƒ…å†µ |
|-----------------------|----------------|----------------|----------|
| Dockeré•œåƒå¤§å°        | â‰¤4GB           | â‰ˆ2GBï¼ˆä¼˜åŒ–åï¼‰ | âœ…        |
| è¿è¡Œæ—¶å†…å­˜            | â‰¤900MB         | â‰ˆ200-300MB     | âœ…        |
| é¢„æµ‹æ—¶é—´ï¼ˆ1ä¸‡æ¡æ•°æ®ï¼‰ | æ— æ˜ç¡®é™åˆ¶     | â‰ˆ200ç§’         | âœ…ï¼ˆ5%è¯„åˆ†å½±å“å°ï¼‰ |


#### 4.3 ä»£ç å®ç°ï¼ˆFastAPIéƒ¨ç½²ï¼‰
##### ï¼ˆ1ï¼‰æ¨¡å‹åŠ è½½ä¸é¢„æµ‹
```python
# sentiment_model.py
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer

class LightweightSentimentModel:
    def __init__(self):
        # åŠ è½½è½»é‡çº§é¢„è®­ç»ƒæ¨¡å‹ï¼ˆæœ¬åœ°ä¸‹è½½ï¼Œé¿å…å®¹å™¨è”ç½‘ï¼‰
        self.model_name = "distilbert-base-uncased-finetuned-sst-2-english"
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)
        self.pipe = pipeline(
            "sentiment-analysis",
            model=self.model,
            tokenizer=self.tokenizer,
            return_all_scores=True
        )

    def predict(self, news_text: str) -> dict:
        # æ¨¡å‹é¢„æµ‹ï¼ˆæ˜ å°„ä¸º-1/1æ ‡ç­¾ï¼‰
        result = self.pipe(news_text)[0]
        positive_prob = result[1]['score']  # æ­£é¢æ¦‚ç‡
        negative_prob = result[0]['score']  # è´Ÿé¢æ¦‚ç‡
        sentiment = 1 if positive_prob > negative_prob else -1
        return {
            "sentiment": str(sentiment),
            "probability": str(max(positive_prob, negative_prob))
        }
```

##### ï¼ˆ2ï¼‰FastAPIæœåŠ¡ï¼ˆè½»é‡ã€é«˜æ€§èƒ½ï¼‰
```python
# main.pyï¼ˆAPIéƒ¨ç½²æ–‡ä»¶ï¼‰
from fastapi import FastAPI
from pydantic import BaseModel
from sentiment_model import LightweightSentimentModel

app = FastAPI()
model = LightweightSentimentModel()  # åˆå§‹åŒ–æ¨¡å‹ï¼ˆä»…åŠ è½½1æ¬¡ï¼‰

# å®šä¹‰è¾“å…¥æ ¼å¼
class NewsText(BaseModel):
    news_text: str

# æƒ…æ„Ÿåˆ†ææ¥å£ï¼ˆç¬¦åˆTask2è¦æ±‚çš„ç«¯ç‚¹ï¼‰
@app.post("/predict_sentiment")
def predict_sentiment(data: NewsText):
    result = model.predict(data.news_text)
    return result

# å¯åŠ¨å‘½ä»¤ï¼šuvicorn main:app --host 0.0.0.0 --port 5724
```


#### 4.4 Dockeré•œåƒä¼˜åŒ–ï¼ˆå…³é”®ï¼æ§åˆ¶å¤§å°ï¼‰
é‡‡ç”¨**å¤šé˜¶æ®µæ„å»º**ï¼Œä»…ä¿ç•™è¿è¡Œå¿…éœ€çš„æ–‡ä»¶ï¼Œé¿å…å†—ä½™ä¾èµ–ï¼š
```dockerfile
# Dockerfile
# ç¬¬ä¸€é˜¶æ®µï¼šæ„å»ºç¯å¢ƒï¼ˆä¸‹è½½æ¨¡å‹+å®‰è£…ä¾èµ–ï¼‰
FROM python:3.9-slim AS builder
WORKDIR /app

# å®‰è£…ä¾èµ–ï¼ˆ--no-cache-dirå‡å°‘ç¼“å­˜ï¼‰
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼ˆå®¹å™¨è¿è¡Œæ—¶ä¸è”ç½‘ï¼‰
RUN python -c "from transformers import AutoTokenizer, AutoModelForSequenceClassification; \
    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english'); \
    model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english'); \
    tokenizer.save_pretrained('./model'); \
    model.save_pretrained('./model')"

# ç¬¬äºŒé˜¶æ®µï¼šè¿è¡Œç¯å¢ƒï¼ˆä»…å¤åˆ¶å¿…éœ€æ–‡ä»¶ï¼‰
FROM python:3.9-slim
WORKDIR /app

# å¤åˆ¶ä¾èµ–ï¼ˆä»æ„å»ºé˜¶æ®µï¼‰
COPY --from=builder /usr/local/lib/python3.9/site-packages /usr/local/lib/python3.9/site-packages
COPY --from=builder /app/model /app/model  # å¤åˆ¶æœ¬åœ°æ¨¡å‹

# å¤åˆ¶ä»£ç 
COPY main.py /app/
COPY sentiment_model.py /app/

# æš´éœ²ç«¯å£ï¼ˆç¬¦åˆTask2è¦æ±‚çš„5724ï¼‰
EXPOSE 5724

# å¯åŠ¨APIï¼ˆç¦ç”¨GPUï¼Œä»…ç”¨CPUï¼‰
ENV TRANSFORMERS_OFFLINE=1  # ç¦»çº¿æ¨¡å¼ï¼ˆä¸è”ç½‘ä¸‹è½½æ¨¡å‹ï¼‰
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "5724", "--workers", "2"]
```

`requirements.txt`ï¼ˆä»…ä¿ç•™å¿…éœ€ä¾èµ–ï¼‰ï¼š
```
fastapi==0.104.1
uvicorn==0.24.0
transformers==4.35.2
torch==2.1.0+cpu  # CPUç‰ˆæœ¬PyTorchï¼Œæ¯”GPUç‰ˆæœ¬å°80%
pydantic==2.5.2
```

**é•œåƒæ„å»ºå‘½ä»¤**ï¼š
```bash
docker build -t sentiment-api:v1 .
# æ„å»ºåé•œåƒå¤§å°â‰ˆ2GBï¼Œæ»¡è¶³â‰¤4GBçº¦æŸ
```


### äº”ã€è¿‡æ‹Ÿåˆè§£å†³æ–¹æ¡ˆ
è‹¥ä¼˜åŒ–åä»æ‹…å¿ƒè¿‡æ‹Ÿåˆï¼Œå¯é€šè¿‡ä»¥ä¸‹3ä¸ªæ–¹å‘éªŒè¯ä¸ç¼“è§£ï¼š
1. **éªŒè¯è¿‡æ‹Ÿåˆä¸å¦**ï¼šå¯¹æ¯”è®­ç»ƒé›†ä¸éªŒè¯é›†F1ï¼Œè‹¥å·®è·â‰¤2%ï¼ˆå¦‚è®­ç»ƒ84%ã€éªŒè¯82%ï¼‰ï¼Œæ— æ˜æ˜¾è¿‡æ‹Ÿåˆï¼›è‹¥å·®è·â‰¥5%ï¼Œéœ€è°ƒæ•´ã€‚
2. **å¢å¼ºæ­£åˆ™åŒ–**ï¼šå‡å°çº¿æ€§æ¨¡å‹çš„`C`ï¼ˆå¦‚LogRegçš„Cä»2.0â†’1.0ï¼‰ï¼Œæˆ–å¢åŠ SGDClassifierçš„`alpha`ï¼ˆä»0.001â†’0.01ï¼‰ã€‚
3. **æ–‡æœ¬æ•°æ®å¢å¼º**ï¼šå¯¹è®­ç»ƒé›†è¿›è¡ŒåŒä¹‰è¯æ›¿æ¢ï¼ˆå¦‚â€œriseâ€â†’â€œincreaseâ€ï¼Œç”¨`nltk.wordnet`ï¼‰ï¼Œé¿å…ä¿®æ”¹é‡‘èæœ¯è¯­ï¼ˆå¦‚â€œeurâ€â€œepsâ€ï¼‰ï¼Œä»£ç ç¤ºä¾‹ï¼š
```python
from nltk.corpus import wordnet
import random

def augment_text(text):
    words = text.split()
    augmented_words = []
    for word in words:
        # ä»…æ›¿æ¢æƒ…æ„Ÿè¯ï¼Œä¸æ›¿æ¢é‡‘èæœ¯è¯­
        if word in positive_words or word in negative_words:
            synonyms = [syn.lemma() for syn in wordnet.synsets(word) if syn.pos() == 'v']
            if synonyms and random.random() < 0.3:  # 30%æ¦‚ç‡æ›¿æ¢
                augmented_words.append(random.choice(synonyms))
            else:
                augmented_words.append(word)
        else:
            augmented_words.append(word)
    return ' '.join(augmented_words)

# å¯¹è®­ç»ƒé›†åº”ç”¨å¢å¼ºï¼ˆæ‰©å¤§æ•°æ®é‡ï¼‰
df_train['augmented_title'] = df_train['news_title'].apply(augment_text)
df_augmented = df_train.copy()
df_augmented['news_title'] = df_augmented['augmented_title']
df_train = pd.concat([df_train, df_augmented], ignore_index=True)  # æ•°æ®é‡ç¿»å€
```


### å…­ã€æ–¹æ¡ˆä¼˜å…ˆçº§ä¸è½åœ°å»ºè®®
| æ–¹æ¡ˆ                | é¢„æœŸF1  | éƒ¨ç½²å¤æ‚åº¦ | èµ„æºå ç”¨ | æ¨èä¼˜å…ˆçº§ |
|---------------------|---------|------------|----------|------------|
| ä¼˜åŒ–çº¿æ€§æ¨¡å‹+ç‰¹å¾   | 82%-83% | ä½         | æä½     | 1ï¼ˆé¦–é€‰ï¼‰  |
| è½»é‡çº§Votingé›†æˆ    | 83%-84% | ä¸­         | ä½       | 2ï¼ˆæ¬¡é€‰ï¼‰  |
| DistilBERT Tiny     | 85%-88% | ä¸­é«˜       | ä¸­       | 3ï¼ˆè¿›é˜¶ï¼‰  |


#### è½åœ°æ­¥éª¤å»ºè®®
1. **ç¬¬ä¸€æ­¥**ï¼šå…ˆä¼˜åŒ–ç‰¹å¾å·¥ç¨‹ï¼ˆæŒ‰2.1ä¿®æ”¹`handcrafted_features.py`å’Œ`text_preprocessor.py`ï¼‰ï¼Œé‡æ–°è®­ç»ƒLogistic Regression/LinearSVCï¼ŒéªŒè¯æ•ˆæœã€‚
2. **ç¬¬äºŒæ­¥**ï¼šè‹¥F1<82%ï¼ŒåŠ å…¥Votingé›†æˆï¼ˆæŒ‰3.2å®ç°ï¼‰ï¼Œæå‡ç¨³å®šæ€§ã€‚
3. **ç¬¬ä¸‰æ­¥**ï¼šè‹¥éœ€æ›´é«˜æ•ˆæœï¼Œå°è¯•DistilBERT Tinyï¼ˆæŒ‰4.3-4.4éƒ¨ç½²ï¼‰ï¼Œé‡ç‚¹ä¼˜åŒ–Dockeré•œåƒã€‚
4. **éƒ¨ç½²æµ‹è¯•**ï¼šæœ¬åœ°å¯åŠ¨APIåï¼Œç”¨`curl`æµ‹è¯•å“åº”æ—¶é—´å’Œå†…å­˜ï¼š
   ```bash
   # æµ‹è¯•API
   curl -X POST "http://localhost:5724/predict_sentiment" -H "Content-Type: application/json" -d '{"news_text": "Eur 100m profit rise beats forecast"}'
   # ç›‘æ§å†…å­˜ï¼ˆå®¹å™¨åæ›¿æ¢ä¸ºå®é™…åç§°ï¼‰
   docker stats <container_id>
   ```


é€šè¿‡ä»¥ä¸Šæ–¹æ¡ˆï¼Œå¯åœ¨æ»¡è¶³ã€Œè½»é‡éƒ¨ç½²çº¦æŸã€çš„å‰æä¸‹ï¼Œæœ€å¤§åŒ–æƒ…æ„Ÿåˆ†æçš„åŠ æƒF1å¾—åˆ†ï¼ŒåŒæ—¶æ§åˆ¶é¢„æµ‹æ—¶é—´æŸå¤±ï¼Œå…¼é¡¾Task2çš„è¯„åˆ†æƒé‡ï¼ˆF1å 20%ï¼Œæ—¶é—´å 5%ï¼‰ã€‚

## deepseek

æ ¹æ®ä½ çš„é¡¹ç›®æè¿°ã€å½“å‰è¿›å±•å’Œéƒ¨ç½²é™åˆ¶ï¼Œæˆ‘æ¥åˆ†æå¹¶æå‡ºæ”¹è¿›å»ºè®®ï¼š

### ğŸ“Š å½“å‰é—®é¢˜åˆ†æ

1. **æ¨¡å‹æ€§èƒ½ç“¶é¢ˆï¼ˆ80.9% F1ï¼‰**ï¼š
   - ç‰¹å¾è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼ˆTF-IDF + æ‰‹å·¥ç‰¹å¾ï¼‰
   - ä¼ ç»Ÿæœºå™¨å­¦ä¹ å¯¹æ–‡æœ¬è¯­ä¹‰ç†è§£ä¸è¶³
   - æ‰‹å·¥ç‰¹å¾æå‡æœ‰é™ï¼ˆä»…0.0019-0.0036æå‡ï¼‰

2. **éƒ¨ç½²é™åˆ¶æŒ‘æˆ˜**ï¼š
   - å†…å­˜ â‰¤ 900MBï¼Œé•œåƒ â‰¤ 4GB
   - æ— GPU/ç½‘ç»œè®¿é—®
   - é¢„æµ‹æ—¶é—´å½±å“è¯„åˆ†ï¼ˆ5%æƒé‡ï¼‰

### ğŸš€ è½»é‡çº§æ”¹è¿›æ–¹æ¡ˆæ¨è

#### æ–¹æ¡ˆä¸€ï¼šFastTextï¼ˆæœ€ä½³å¹³è¡¡ï¼‰
```python
# fasttext_try.py
import fasttext
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score

# æ•°æ®æ ¼å¼è½¬æ¢ï¼š__label__1 text...
def prepare_fasttext_format(df, text_col='news_title', label_col='sentiment'):
    texts = []
    for idx, row in df.iterrows():
        label = 1 if row[label_col] == 1 else -1
        text = row[text_col].replace('\n', ' ').replace('\r', ' ')
        texts.append(f"__label__{label} {text}")
    return '\n'.join(texts)

# äº¤å‰éªŒè¯è®­ç»ƒ
def fasttext_cv(df, n_folds=5):
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)
    scores = []

    for fold, (train_idx, val_idx) in enumerate(skf.split(df['news_title'], df['sentiment'])):
        train_df = df.iloc[train_idx]
        val_df = df.iloc[val_idx]

        # å‡†å¤‡æ•°æ®
        train_data = prepare_fasttext_format(train_df)
        with open(f'fasttext_train_fold{fold}.txt', 'w') as f:
            f.write(train_data)

        # è®­ç»ƒæ¨¡å‹ï¼ˆè°ƒå‚é‡ç‚¹ï¼‰
        model = fasttext.train_supervised(
            input=f'fasttext_train_fold{fold}.txt',
            lr=0.8,            # å­¦ä¹ ç‡
            epoch=50,          # è¿­ä»£æ¬¡æ•°
            wordNgrams=2,      # n-gramç‰¹å¾
            dim=100,           # è¯å‘é‡ç»´åº¦
            loss='softmax',    # æŸå¤±å‡½æ•°
            minCount=2,        # æœ€å°è¯é¢‘
            verbose=2
        )

        # é¢„æµ‹
        predictions = []
        for text in val_df['news_title']:
            pred = model.predict(text, k=1)[0][0]
            pred_label = 1 if '__label__1' in pred else -1
            predictions.append(pred_label)

        # è®¡ç®—F1
        f1 = f1_score(val_df['sentiment'], predictions, average='weighted')
        scores.append(f1)

    return np.mean(scores), scores
```

**ä¼˜åŠ¿**ï¼š
- æ¨¡å‹æå°ï¼ˆé€šå¸¸<100MBï¼‰
- è®­ç»ƒå’Œæ¨ç†æå¿«
- å†…ç½®n-gramç‰¹å¾
- å¯¹çŸ­æ–‡æœ¬æ•ˆæœå¥½
- æ”¯æŒå¢é‡è®­ç»ƒ

#### æ–¹æ¡ˆäºŒï¼šDistilBERT + è’¸é¦ï¼ˆè½»é‡BERTï¼‰
```python
# distilbert_try.py
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from transformers import Trainer, TrainingArguments
import torch

class DistilBertSentimentClassifier:
    def __init__(self, model_name='distilbert-base-uncased'):
        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)
        self.model = DistilBertForSequenceClassification.from_pretrained(
            model_name,
            num_labels=2,
            output_attentions=False,
            output_hidden_states=False
        )

    def train(self, train_texts, train_labels, val_texts, val_labels):
        # ç²¾ç®€è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir='./results',
            num_train_epochs=3,           # å°‘é‡epochs
            per_device_train_batch_size=16,
            per_device_eval_batch_size=16,
            warmup_steps=100,
            weight_decay=0.01,
            logging_dir='./logs',
            logging_steps=10,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            fp16=False,  # ä¸ä½¿ç”¨æ··åˆç²¾åº¦èŠ‚çœå†…å­˜
            gradient_accumulation_steps=1,
        )

        # è®­ç»ƒ...
```

**ä¼˜åŠ¿**ï¼š
- ç›¸æ¯”BERTä½“ç§¯å°40%ï¼Œé€Ÿåº¦å¿«60%
- ä¿æŒ90%ä»¥ä¸Šçš„BERTæ€§èƒ½
- æ¨¡å‹å¤§å°~250MB
- æ¨ç†æ—¶é—´å¿«

#### æ–¹æ¡ˆä¸‰ï¼šEnsemble of Simple Modelsï¼ˆé›†æˆå­¦ä¹ ï¼‰
```python
# ensemble_try.py
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import VotingClassifier, StackingClassifier
from sklearn.feature_extraction.text import TfidfVectorizer

def create_lightweight_ensemble():
    # 1. TF-IDFç‰¹å¾ï¼ˆé€‚åº¦è§„æ¨¡ï¼‰
    vectorizer = TfidfVectorizer(
        max_features=2000,
        ngram_range=(1, 2),
        min_df=3,
        max_df=0.9,
        sublinear_tf=True
    )

    # 2. è½»é‡çº§åŸºæ¨¡å‹
    models = [
        ('nb', MultinomialNB(alpha=0.1)),
        ('lr', LogisticRegression(
            C=1.0,
            solver='liblinear',
            max_iter=500,
            penalty='l2'
        )),
        ('svm_linear', LinearSVC(
            C=0.5,
            penalty='l2',
            dual=False,
            max_iter=1000
        ))
    ]

    # 3. è½¯æŠ•ç¥¨é›†æˆ
    ensemble = VotingClassifier(
        estimators=models,
        voting='soft',
        weights=[1, 1.5, 1]  # è°ƒæ•´æƒé‡
    )

    return Pipeline([
        ('tfidf', vectorizer),
        ('ensemble', ensemble)
    ])
```

### ğŸ“ˆ å…·ä½“å®æ–½å»ºè®®

#### é˜¶æ®µä¸€ï¼šå¿«é€ŸéªŒè¯ï¼ˆ1-2å¤©ï¼‰
1. **å…ˆè¯•FastText**ï¼š
   ```bash
   pip install fasttext
   # ç®€å•åŸºçº¿ï¼Œçœ‹æ˜¯å¦èƒ½è¶…è¿‡80.9%
   ```

2. **å‚æ•°è°ƒä¼˜é‡ç‚¹**ï¼š
   - `wordNgrams`: [2, 3, 4] (å¯¹æ ‡é¢˜å¾ˆé‡è¦)
   - `dim`: [50, 100, 200] (ç»´åº¦è¶Šå°è¶Šå¿«)
   - `epoch`: [20, 30, 50]
   - `lr`: [0.5, 0.8, 1.0]

#### é˜¶æ®µäºŒï¼šç‰¹å¾å·¥ç¨‹å¢å¼ºï¼ˆå¹¶è¡Œï¼‰
```python
# enhanced_features.py
def create_enhanced_features(df):
    """
    åŸºäºEDAçš„æ·±åº¦ç‰¹å¾å·¥ç¨‹
    """
    features = []

    for text in df['news_title']:
        text_lower = str(text).lower()

        # 1. é¢†åŸŸç‰¹å®šè¯å…¸ï¼ˆåŸºäºä½ çš„é«˜é¢‘è¯ï¼‰
        finance_pos_words = ['profit', 'dividend', 'growth', 'expansion']
        finance_neg_words = ['loss', 'cut', 'downgrade', 'warn']

        # 2. æƒ…æ„Ÿå¼ºåº¦ç‰¹å¾
        strong_pos = ['soar', 'surge', 'skyrocket', 'rocket']
        strong_neg = ['plummet', 'collapse', 'crash', 'plunge']

        # 3. å…¬å¸/è¡Œä¸šç‰¹å®šç‰¹å¾
        company_indicators = ['finnish', 'malaysia', 'sen', 'eur']

        # 4. æ ‡é¢˜ç»“æ„ç‰¹å¾
        has_colon = ':' in text
        has_question = '?' in text
        starts_with_company = any(text_lower.startswith(word) for word in company_indicators)

        # 5. æ•°å­—ç‰¹å¾
        import re
        numbers = re.findall(r'\d+', text)
        has_percent = '%' in text or 'percent' in text_lower
        has_million = 'million' in text_lower or 'm' in text_lower

        features.append([
            # åŸæœ‰ç‰¹å¾...
            int(has_colon), int(has_question), int(starts_with_company),
            int(has_percent), int(has_million), len(numbers)
        ])

    return features
```

#### é˜¶æ®µä¸‰ï¼šæ¨¡å‹ä¼˜åŒ–ç­–ç•¥
1. **æ•°æ®å¢å¼º**ï¼ˆæœ‰é™æ•°æ®æ—¶ç‰¹åˆ«æœ‰æ•ˆï¼‰ï¼š
   ```python
   def augment_text(text):
       # åŒä¹‰è¯æ›¿æ¢ï¼ˆä½¿ç”¨è½»é‡çº§åº“ï¼‰
       synonyms = {
           'profit': ['earnings', 'gain', 'income'],
           'loss': ['deficit', 'shortfall', 'decline'],
           # ...
       }
       # ç®€å•æ•°æ®å¢å¼º
       return text
   ```

2. **ç±»åˆ«å¹³è¡¡**ï¼š
   ```python
   from imblearn.over_sampling import RandomOverSampler
   from imblearn.under_sampling import RandomUnderSampler
   
   # æˆ–ä½¿ç”¨class_weightå‚æ•°
   model = LogisticRegression(class_weight='balanced')
   ```

#### é˜¶æ®µå››ï¼šéƒ¨ç½²ä¼˜åŒ–
1. **æ¨¡å‹å‹ç¼©**ï¼š
   ```python
   # å¯¹äºé€»è¾‘å›å½’
   def compress_model(model, feature_names, top_n=100):
       # åªä¿ç•™æœ€é‡è¦çš„ç‰¹å¾
       coef = model.coef_[0]
       important_idx = np.argsort(np.abs(coef))[-top_n:]
       # é‡æ–°è®­ç»ƒç²¾ç®€æ¨¡å‹
   ```

2. **ç¼“å­˜æœºåˆ¶**ï¼š
   ```python
   from functools import lru_cache
   
   @lru_cache(maxsize=1000)
   def predict_cached(text):
       return model.predict([text])[0]
   ```

### ğŸ¯ ä¼˜å…ˆçº§å»ºè®®

æŒ‰ä»¥ä¸‹é¡ºåºå°è¯•ï¼š

1. **ç«‹å³å°è¯•**ï¼šFastTextï¼ˆæœ€è½»é‡ï¼Œéƒ¨ç½²æœ€å®¹æ˜“ï¼‰
2. **å¤‡é€‰æ–¹æ¡ˆ**ï¼šè½»é‡çº§é›†æˆï¼ˆLogisticRegression + NaiveBayes + LinearSVCï¼‰
3. **æ€§èƒ½è¿½æ±‚**ï¼šDistilBERTï¼ˆå¦‚æœFastTextè¾¾ä¸åˆ°82%+ï¼‰
4. **æè‡´è½»é‡**ï¼šONNX Runtime + é‡åŒ–æ¨¡å‹

### ğŸ“ é¢„æœŸæ”¶ç›Š

| æ–¹æ³• | é¢„æœŸF1æå‡ | å†…å­˜å ç”¨ | æ¨ç†æ—¶é—´ | éƒ¨ç½²å¤æ‚åº¦ |
|------|-----------|----------|----------|------------|
| FastText | +1-3% | <100MB | <10ms | ä½ |
| è½»é‡é›†æˆ | +0.5-2% | 200-400MB | <20ms | ä¸­ |
| DistilBERT | +2-5% | 300-500MB | 50-100ms | ä¸­é«˜ |

**æ¨èè·¯çº¿**ï¼šä»FastTextå¼€å§‹ï¼Œå¦‚æœæ•ˆæœè¾¾åˆ°82%+å°±é‡‡ç”¨ï¼Œå¦åˆ™å°è¯•è½»é‡é›†æˆï¼Œæœ€åè€ƒè™‘DistilBERTã€‚

### ğŸ› ï¸ å¿«é€Ÿå¯åŠ¨ä»£ç 

```python
# quick_start.py
import fasttext
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score

# 1. åŠ è½½æ•°æ®
df = pd.read_excel('training_news-sentiment.xlsx')

# 2. å‡†å¤‡FastTextæ ¼å¼
def to_fasttext_format(row):
    label = '__label__1' if row['sentiment'] == 1 else '__label__-1'
    return f"{label} {row['news_title']}\n"

with open('fasttext_data.txt', 'w') as f:
    for _, row in df.iterrows():
        f.write(to_fasttext_format(row))

# 3. è®­ç»ƒ
model = fasttext.train_supervised(
    input='fasttext_data.txt',
    lr=0.8,
    epoch=30,
    wordNgrams=2,
    dim=100,
    verbose=2
)

# 4. æµ‹è¯•
test_texts = ["Finnish group sees profit rise"]
predictions = model.predict(test_texts)
print(predictions)
```

å»ºè®®ä½ å…ˆç”¨è¿™ä¸ªç®€å•è„šæœ¬æµ‹è¯•FastTextçš„åŸºçº¿æ€§èƒ½ï¼Œç„¶åå†³å®šåç»­æ–¹å‘ã€‚

---

## åŒä¹‰è¯æ•°æ®å¢å¼º

```
def augment_text(text):
    # åŒä¹‰è¯æ›¿æ¢ï¼ˆä½¿ç”¨è½»é‡çº§åº“ï¼‰
    synonyms = {
        'profit': ['earnings', 'gain', 'income'],
        'loss': ['deficit', 'shortfall', 'decline'],
        'pct': ['percent', 'percentage', '%'],
    }
    # ç®€å•æ•°æ®å¢å¼º
    return text
```

3. **æ–‡æœ¬æ•°æ®å¢å¼º**ï¼šå¯¹è®­ç»ƒé›†è¿›è¡ŒåŒä¹‰è¯æ›¿æ¢ï¼ˆå¦‚â€œriseâ€â†’â€œincreaseâ€ï¼Œç”¨`nltk.wordnet`ï¼‰ï¼Œé¿å…ä¿®æ”¹é‡‘èæœ¯è¯­ï¼ˆå¦‚â€œeurâ€â€œepsâ€ï¼‰ï¼Œä»£ç ç¤ºä¾‹ï¼š
```python
from nltk.corpus import wordnet
import random

def augment_text(text):
    words = text.split()
    augmented_words = []
    for word in words:
        # ä»…æ›¿æ¢æƒ…æ„Ÿè¯ï¼Œä¸æ›¿æ¢é‡‘èæœ¯è¯­
        if word in positive_words or word in negative_words:
            synonyms = [syn.lemma() for syn in wordnet.synsets(word) if syn.pos() == 'v']
            if synonyms and random.random() < 0.3:  # 30%æ¦‚ç‡æ›¿æ¢
                augmented_words.append(random.choice(synonyms))
            else:
                augmented_words.append(word)
        else:
            augmented_words.append(word)
    return ' '.join(augmented_words)

# å¯¹è®­ç»ƒé›†åº”ç”¨å¢å¼ºï¼ˆæ‰©å¤§æ•°æ®é‡ï¼‰
df_train['augmented_title'] = df_train['news_title'].apply(augment_text)
df_augmented = df_train.copy()
df_augmented['news_title'] = df_augmented['augmented_title']
df_train = pd.concat([df_train, df_augmented], ignore_index=True)  # æ•°æ®é‡ç¿»å€
```

## distilbert trials

### baseline

- https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english/blob/main/config.json | config.json &middot; distilbert/distilbert-base-uncased-finetuned-sst-2-english at main

### method

- https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis | mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis &middot; Hugging Face
- https://huggingface.co/spaces/sway0604/news_sentiment | News Sentiment - a Hugging Face Space by sway0604
- https://www.kaggle.com/code/dhaouadiibtihel98/fine-tuning-distilbert-for-sentiment-analysis | Fine-Tuning DistilBERT for Sentiment Analysis
- https://www.kaggle.com/code/joshplnktt/sentiment-analysis-w-distilbert | Sentiment Analysis w/ DistilBERT
- https://www.kaggle.com/code/ocanaydin/financial-sentiment-bert | financial_sentiment_BERT
- https://github.com/vedavyas0105/Financial-Sentiment-Distillation | vedavyas0105/Financial-Sentiment-Distillation: This project leverages knowledge distillation to create a lightweight yet powerful sentiment analysis model, tailored specifically for financial news data. Using a teacher-student approach, the project distills knowledge from a large FinBERT model into a compact DistilBERT-based student model, balancing performance and efficiency.
- https://medium.com/@choudhary.man/fine-tuning-distilbert-for-financial-sentiment-analysis-a-practical-implementation-d6df80e8340f | Fine-Tuning DistilBERT for Financial Sentiment Analysis: A Practical Implementation | by Manish Bansilal Choudhary | Medium
- https://github.com/Ramy-Abdulazziz/Financial-Sentiment-Analysis | Ramy-Abdulazziz/Financial-Sentiment-Analysis: LLM's trained and fine tuned for financial sentiment analysis
- https://huggingface.co/AdityaAI9/distilbert_finance_sentiment_analysis#:~:text=A%20fine-tuned%20DistilBERT%20model%20for%20financial%20text%20sentiment,statements%20into%20three%20categories%3A%20positive%2C%20negative%2C%20and%20neutral. | AdityaAI9/distilbert_finance_sentiment_analysis &middot; Hugging Face

## dataset

- https://huggingface.co/datasets/takala/financial_phrasebank | takala/financial_phrasebank &middot; Datasets at Hugging Face
- https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment | zeroshot/twitter-financial-news-sentiment &middot; Datasets at Hugging Face
